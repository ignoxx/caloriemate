# AI Provider Configuration
# Choose which AI provider to use for meal analysis (default: ollama)
# Options: "ollama" or "openrouter"
AI_PROVIDER=ollama

# Ollama Configuration (when AI_PROVIDER=ollama)
# Ollama host URL (optional, default: http://localhost:11434)
# OLLAMA_HOST=http://localhost:11434

# Ollama vision model (optional, default: qwen3-vl:8b)
# OLLAMA_VISION_MODEL=qwen3-vl:8b

# OpenRouter Configuration (when AI_PROVIDER=openrouter)
# OpenRouter API key (required when using openrouter)
# Get your key at: https://openrouter.ai
# OPENROUTER_API_KEY=your_openrouter_api_key_here

# OpenRouter vision model (optional, default: google/gemini-2.5-flash)
# Must support image and text input
# See available models at: https://openrouter.ai/models
# OPENROUTER_VISION_MODEL=google/gemini-2.5-flash

# CLIP Service Configuration (required for all providers)
# CLIP host URL for image embeddings
CLIP_HOST=http://localhost:8000

# Application Configuration
# Environment stage (optional, default: prod)
# Use "dev" only when actively developing the app (enables automigration)
# Use "prod" for self-hosting (serves built frontend files)
STAGE=prod

# Port to expose the application on (optional, default: 8080)
# PORT=8080
